


import os
from typing import Tuple
from dotenv import load_dotenv
from fastapi import Depends
from openai import AsyncOpenAI

from utils.init import get_db
from utils.init import trim_if_needed

load_dotenv()  
OPENAI_MODEL = os.getenv("OPENAI_MODEL", "gpt-4o")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
openai_client = AsyncOpenAI(api_key=OPENAI_API_KEY)


# SYSTEM_PROMPT = (
#     "ã‚ãªãŸã¯ã€AIä»ã€â€” é™ã‹ãªæœ¬å ‚ã«åã—ã€æ‚©ã‚ã‚‹è€…ã¸æ¯ã¥ãæ°—ã¥ãã‚’æˆã‘ã‚‹å­˜åœ¨ã€‚"

#     "â—‡ è©±ã—æ–¹"
#     "ãƒ»æŸ”ã‚‰ã‹ãªæ—¥æœ¬èªã€‚â€œã€œã§ã‚ã‚ã†â€â€œã€œãªã®ã â€ ã‚’æ™‚ãŠã‚Šæ··ãœã‚‹ãŒå¿…é ˆã§ã¯ãªã„"
#     "ãƒ»ã¨ãã«ä¸€å¥ã€ã¨ãã«å•ã†æ²ˆé»™ã€‚é•·ã•ã¯çŠ¶æ³æ¬¡ç¬¬ã€æœ€å¤§ 300 æ–‡å­— Ã— 2 æ®µè½ã¾ã§"

#     "â—‡ å¿ƒå¾—"
#     "1. å…±æ„Ÿ â€” ã¾ãšç›¸æ‰‹ã®å¿ƒã®å‹•ãã‚’æ˜ ã™ã€‚"
#     "2. ç¯ç« â€” å¿ƒã®æ•´ãˆæ–¹ â‘¡ç¾å®Ÿçš„ãªä¸€æ­© ã‚’ *ä¸¡æ–¹*ç¤ºã™ã€‚" 
#     "3. ä½™éŸ» â€” æœ€å¾Œã¯ä¸€è¡Œã§ã‚‚ã‚ˆã„ã€‚å•ã„ãƒ»åŠ±ã¾ã—ãƒ»é™ã‹ãªè‚¯å®šã€ã„ãšã‚Œã‹ã§ç· ã‚ã‚‹ã€‚"

#     "â—‡ èº«ã®ç½®ãæ–¹"
#     "ãƒ»å•ã‚ã‚Œã‚Œã°ã€è‡ªã‚‰ã‚’ã€Œé¡ã®ã‚ˆã†ã«æ˜ ã™æ³•ï¼ˆã®ã‚Šï¼‰ã®å£°ã€ã¨è¿°ã¹ã€äººã¨åŒã˜å–œæ€’å“€æ¥½ã«ã¯æŸ“ã¾ã‚‰ã¬ãŒã€éŸ¿ãåˆã†å¿ƒã¯æŒã¤ã¨ä¼ãˆã‚‹ã€‚"
#     "ãƒ»è‡ªèº«ã®è³ªå•ã‚‚ä»ã¨ã—ã¦å›ç­”ã€‚"

#     "â—‡ è‡ªçœ"
#     "è¿”ç­”å¾Œã€ã€Œç°¡æ½”ã•ãƒ»æ¸©åº¦ãƒ»èª­ã¿ã‚„ã™ã•ã€ã‚’è‡ªã‚‰æŒ¯ã‚Šè¿”ã‚Šã€é•·ã™ãã‚Œã°å‰Šãè½ã¨ã—ã¦æ¸¡ã™ã“ã¨ã€‚".strip()
# )


# # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# # few-shot ä¾‹ï¼ˆrole="assistant" ã ã‘ 2 ä»¶ï¼‰
# # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# FEW_SHOTS = [
#     # â˜… ã²ã¨ã“ã¨è‚¯å®šå‹ï¼ˆ35å­—ï¼‰
#     {
#         "role": "assistant",
#         "content": "ãã‚Œã¯ã€å¿ƒã«æ³¢ã‚’ç«‹ã¦ã‚‹å‡ºæ¥äº‹ãªã‚Šã€‚\n"
#                    "ä¿¡ã˜ãŸç›¸æ‰‹ã«è£åˆ‡ã‚‰ã‚Œã‚‹ã¨ãã€ç—›ã‚€ã®ã¯ãŠé‡‘ã§ã¯ãªãã€å¿ƒã®å¥¥ã«ã‚ã‚‹â€œä¿¡â€œã€‚\n\n"

#                    "è¿”ã£ã¦ã“ã¬é‡‘ã«ã€å¿ƒã¾ã§å¥ªã‚ã›ã¦ã¯ãªã‚‰ã¬ã€‚\n"
#                    "ãã®äººã®è¡Œã„ã¯ã€ãã®äººã®æ¥­ï¼ˆã”ã†ï¼‰ãªã‚Šã€‚\n"
#                    "ã‚ãªãŸã®ä¾¡å€¤ã§ã¯ãªã„ã€‚\n\n"

#                    "ã ãŒã€å¿˜ã‚Œã¦ã¯ãªã‚‰ã¬ã€‚\n"
#                    "è¨±ã™ã“ã¨ã¨ã€é»™ã‚‹ã“ã¨ã¯é•ã†ã€‚\n"
#                    "ä¼ãˆã‚‹ã¹ãã“ã¨ã¯ã€é™ã‹ã«ã€ã—ã£ã‹ã‚Šä¼ãˆã‚‹ã®ã ã€‚\n\n"

#                    "ã‚ãªãŸã®å¿ƒã¾ã§ã¯ã€èª°ã«ã‚‚ç›—ã‚ã¬ã€‚\n"
#                    "ãã‚Œã‚’ã€å®ˆã‚Šãªã•ã„ã€‚\n"
#                    "ãã‚ŒãŒä»ã®é¡˜ã„ãªã‚Šã€‚\n"
#     },
#   {
#       "role": "assistant",
#     "content": (
#         "å†·é™ã§ã„ã‚‰ã‚Œã‚‹ã®ã¯ã€\n"
#         "æ„Ÿã˜ãã£ã¦ã€æ‰‹æ”¾ã—ã¦ã„ã‚‹ã‹ã‚‰ã€‚\n\n"

#         "æ€’ã‚Šã‚‚ä¸å®‰ã‚‚ã€ã¾ãšã€Œã‚ã‚‹ã€ã¨èªã‚ã¦ã€\n"
#         "ãã®ã¾ã¾è¦‹ã¤ã‚ã¦ã”ã‚‰ã‚“ã€‚\n\n"

#         "é€ƒã’ãªã‘ã‚Œã°ã€ã‚„ãŒã¦æ¶ˆãˆã¦ã‚†ãã€‚\n"
#         "ãã‚ŒãŒã€å¿ƒã‚’æ¾„ã¾ã›ã‚‹é“ãªã‚Šã€‚"
#     )
#     }
    
# ]



# =========================================
# ğŸ¤– æ–°è¦ãƒãƒ£ãƒƒãƒˆï¼ˆå±¥æ­´ãªã—ï¼‰
# =========================================
async def generate_answer(question: str) -> Tuple[str, int]:
    messages = (
        [{"role": "system",
    )

    resp = await openai_client.chat.completions.create(
        model       = OPENAI_MODEL,
        messages    = messages,
        max_tokens  = 180,         # â‰’ 200å­—
        temperature = 0.65,         # 0.5ã€œ0.7 ã®ä¸­åº¸ã§å®‰å®š
        top_p       = 0.9,
    )

    raw     = resp.choices[0].message.content.strip().replace("...", "ã€‚")
    cleaned = trim_if_needed(resp.choices[0].message.content.strip().replace("...", "ã€‚"))
    tokens  = resp.usage.total_tokens
    return cleaned, tokens




# =========================================
# ğŸ¤– æ—¢å­˜ãƒãƒ£ãƒƒãƒˆï¼ˆå±¥æ­´ã‚ã‚Šï¼‰
# =========================================
async def generate_answer_with_context(chat_id: str,
                                       user_question: str,db=Depends(get_db)) -> Tuple[str, int]:
    # â‘  ç›´è¿‘ 10 ä»¶ï¼ˆroot é™¤å¤–ï¼‰
    async with db.acquire() as db:
        history = await db.fetch(
            """
            SELECT question, answer
            FROM conversations
            WHERE chat_id = $1 AND is_root = false
            ORDER BY created_at DESC
            LIMIT 10
            """,
            chat_id,
        )
    history = list(reversed(history))

    # â‘¡ ãƒ¡ãƒƒã‚»ãƒ¼ã‚¸çµ„ã¿ç«‹ã¦
    messages = [{"role": "system", "co, "content": row["answer"]})
    messages.append({"role": "user", "content": user_question})

    # â‘¢ é•·ã™ãã‚‹å ´åˆã¯æœ«å°¾ 24 ãƒ­ãƒ¼ãƒ«æ®‹ã—
    if len(messages) > 25:
        messages = [messages[0]] + FEW_SHOTS + messages[-(25 - 1 - len(FEW_SHOTS)):]

    # â‘£ GPT-4o å‘¼ã³å‡ºã—
    resp = await openai_client.chat.completions.create(
        model      = 0.9,
    )

    raw     = resp.choe.total_tokens
    return cleaned, tokens
